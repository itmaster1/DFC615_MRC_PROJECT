{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "배현철_2020517010.ipynb의 사본",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVLU+bLz4BKuJTqQ4m2Xsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itmaster1/DFC615_MRC_PROJECT/blob/main/%EB%B0%B0%ED%98%84%EC%B2%A0_2020517010_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A0cWyu7duhs"
      },
      "source": [
        " MRC 학습 및 파인튜팅을 통한 성능향상\n",
        "\n",
        "```\n",
        "version 1.0 (2021.06.22)\n",
        "created by: 배현철\n",
        "email: itmaster@korea.ac.kr\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTCIkQF8ecDl"
      },
      "source": [
        "#구글 드라이브 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdmfef1TlO3Z"
      },
      "source": [
        "# https://colab.research.google.com/drive/1EebE_t1s3hS6_oAvPH5DiOsLq3WlTTRZ?usp=sharing#scrollTo=OLrvpcBDTP97\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/') \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615') ## 현재 작업 환경으로 설정한 경로를 입력하세요"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF_Zjlb0eqe_"
      },
      "source": [
        "# 1. MRC on [Korqard](https://korquad.github.io/KorQuad%201.0/) with KoElectra\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIVrsxk3gfkM"
      },
      "source": [
        "KorQuAD는 한국어 기반의 기계 독해를 위해 제작한 데이터셋이며, 데이터셋은 지문과 질의 답변으로 이루어져 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mLkHDC5lgfV"
      },
      "source": [
        "import json\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/DFC615')\n",
        "gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
        "!wget https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
        "\n",
        "output_dir = gdrive_path\n",
        "train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "korquad_train = json.load(open(train_file,'r',encoding='utf-8'))    \n",
        "korquad_dev = json.load(open(dev_file,'r',encoding='utf-8'))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_15A18qgvzy"
      },
      "source": [
        "#2. **사전 훈련된 KoELECTRA 활용하여 MRC 해결**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2JcGIPXhweE"
      },
      "source": [
        "사전 훈련된 KoELECTRA를 바탕으로 미세 조정 훈련을 통해 KorQuad 1.0의 질의 응답 문제를 해결"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBKk902vlwzJ"
      },
      "source": [
        "# !pip install sentencepiece\n",
        "!pip install transformers==3.3.1\n",
        "!pip install seqeval\n",
        "!pip install fastprogress\n",
        "!pip install attrdict\n",
        "!pip uninstall pandas\n",
        "!pip install pandas==1.1.5\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZU5wyMKh6T_"
      },
      "source": [
        "#2.1 필요한 라이브러리 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gt0VP8Gol5SD"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from collections import Counter\n",
        "import sys\n",
        "import argparse\n",
        "import string\n",
        "import re\n",
        "import glob\n",
        "import logging\n",
        "import random\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup,\n",
        "    squad_convert_examples_to_features\n",
        ")\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")\n",
        "\n",
        "from transformers import (\n",
        "    ElectraForQuestionAnswering,\n",
        "    XLMRobertaForQuestionAnswering,\n",
        "    \n",
        "    ElectraTokenizer,\n",
        "    XLMRobertaTokenizer,\n",
        "\n",
        "    ElectraConfig,\n",
        "    XLMRobertaConfig\n",
        ")\n",
        "from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor\n",
        "from transformers import pipeline\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#from konlpy.tag import Okt\n",
        "from konlpy.tag import Mecab\n",
        "#tokenizer = Okt()\n",
        "tokenizer_1 = Mecab ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlDIV0zCiHv7"
      },
      "source": [
        "## 2.2 Fine-Tuning을 위한 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFB1_M6el_de"
      },
      "source": [
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "MODEL_FOR_QUESTION_ANSWERING = {\n",
        "    \"koelectra-base-v3\": ElectraForQuestionAnswering,\n",
        "    \"koelectra-small-v3\": ElectraForQuestionAnswering,\n",
        "}\n",
        "TOKENIZER_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraTokenizer,\n",
        "    \"koelectra-small-v3\": ElectraTokenizer,\n",
        "}\n",
        "CONFIG_CLASSES = {\n",
        "    \"koelectra-base-v3\": ElectraConfig,\n",
        "    \"koelectra-small-v3\": ElectraConfig,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6VSgOTeiQ94"
      },
      "source": [
        " --평가스크립트를 기반으로 작성하고 ,Normalize 부분을 조사제거를 위해 코딩 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k_sjyC0mwK6"
      },
      "source": [
        "# Mecab 설치 방법\n",
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk \n",
        "!pip3 install konlpy JPype1-py3\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3G7obRBomLS7"
      },
      "source": [
        "'''KorQuAD v1.0에 대한 공식 평가 스크립트 '''\n",
        "'''본 스크립트는 SQuAD v1.1 평가 스크립트 https://rajpurkar.github.io/SQuAD-explorer/ 를 바탕으로 작성됨.'''\n",
        "def normalize_answer(s):\n",
        "    def remove_(text):\n",
        "        ''' 불필요한 기호 제거 '''\n",
        "        text = re.sub(\"'\", \" \", text)\n",
        "        text = re.sub('\"', \" \", text)\n",
        "        text = re.sub('《', \" \", text)\n",
        "        text = re.sub('》', \" \", text)\n",
        "        text = re.sub('<', \" \", text)\n",
        "        text = re.sub('>', \" \", text)\n",
        "        text = re.sub('〈', \" \", text)\n",
        "        text = re.sub('〉', \" \", text)\n",
        "        text = re.sub(\"\\(\", \" \", text)\n",
        "        text = re.sub(\"\\)\", \" \", text)\n",
        "        text = re.sub(\"‘\", \" \", text)\n",
        "        text = re.sub(\"’\", \" \", text)\n",
        "\n",
        "#        text = re.sub('[0-9]+', '', text)\n",
        "#        text = re.sub('[A-Za-z]+', '', text)\n",
        "        text = re.sub('[^a-zA-Z0-9ㄱ-ㅣ가-힣]', '', text)                    ## 영문,숫자,한글만 제외 하고 제거\n",
        "        text = re.sub('[-=+,#/\\?:^$.@*\\\"※~&%ㆍ·!』\\\\‘’…》]', '', text)  ## 불필요한 특수문자를 추가로 제거\n",
        "\n",
        "        return text\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    def text_preprocessing(text,tokenizer):   ## 정답의 끝에 붙어있는 불필요한 조사를 제거하기 위한 함수정의.\n",
        "        \n",
        "        stopwords = ['을', '를', '이', '가', '은', '는', '의', '로','에', '에서', '과','에게','라는','도','으로','다','라','만','로서','라면서','이렇다']   ## [제거 하고싶은 불용어를 추가]\n",
        "        \n",
        "        token = tokenizer_1.morphs(text)\n",
        "        token = [t for t in token if t not in stopwords]    \n",
        "        return token\n",
        "\n",
        "        #example_pre= \"\".join(text_preprocessing(answer['answer'],tokenizer))        \n",
        "\n",
        "    return \"\".join(text_preprocessing(white_space_fix(remove_punc(lower(remove_(s)))),tokenizer_1))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    # F1 by character\n",
        "    prediction_Char = []\n",
        "    for tok in prediction_tokens:\n",
        "        now = [a for a in tok]\n",
        "        prediction_Char.extend(now)\n",
        "\n",
        "    ground_truth_Char = []\n",
        "    for tok in ground_truth_tokens:\n",
        "        now = [a for a in tok]\n",
        "        ground_truth_Char.extend(now)\n",
        "\n",
        "    common = Counter(prediction_Char) & Counter(ground_truth_Char)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_Char)\n",
        "    recall = 1.0 * num_same / len(ground_truth_Char)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    return {'official_exact_match': exact_match, 'official_f1': f1}\n",
        "\n",
        "\n",
        "def eval_during_train(args, step):\n",
        "    expected_version = 'KorQuAD_v1.0'\n",
        "\n",
        "    dataset_file = os.path.join(args.data_dir, args.predict_file)\n",
        "    prediction_file = os.path.join(args.output_dir, 'predictions_{}.json'.format(step))\n",
        "\n",
        "    with open(dataset_file) as dataset_f:\n",
        "        dataset_json = json.load(dataset_f)\n",
        "        read_version = \"_\".join(dataset_json['version'].split(\"_\")[:-1])\n",
        "        if (read_version != expected_version):\n",
        "            print('Evaluation expects ' + expected_version +\n",
        "                  ', but got dataset with ' + read_version,\n",
        "                  file=sys.stderr)\n",
        "        dataset = dataset_json['data']\n",
        "    with open(prediction_file) as prediction_f:\n",
        "        predictions = json.load(prediction_f)\n",
        "\n",
        "    return evaluate(dataset, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCNZOV5ujy72"
      },
      "source": [
        "## 2.3 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJZ3Vn8BmQDv"
      },
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Train batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 1\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    # Added here for reproductibility\n",
        "    set_seed(args)\n",
        "\n",
        "    for epoch in mb:\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"start_positions\": batch[3],\n",
        "                \"end_positions\": batch[4],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            if args.model_type in [\"xlnet\", \"xlm\"]:\n",
        "                inputs.update({\"cls_index\": batch[5], \"p_mask\": batch[6]})\n",
        "                if args.version_2_with_negative:\n",
        "                    inputs.update({\"is_impossible\": batch[7]})\n",
        "                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n",
        "                    inputs.update(\n",
        "                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args.lang_id).to(args.device)}\n",
        "                    )\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            # model outputs are always tuple in transformers (see doc)\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                    if args.evaluate_during_training:\n",
        "                        results = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "                        for key in sorted(results.keys()):\n",
        "                            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                # Save model checkpoint\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    # Take care of distributed/parallel training\n",
        "                    model_to_save = model.module if hasattr(model, \"module\") else model\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch+1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluation(args, model, tokenizer, global_step=None):\n",
        "    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n",
        "\n",
        "    if not os.path.exists(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(global_step))\n",
        "    logger.info(\"  Num examples = %d\", len(dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if args.model_type in [\"xlm\", \"roberta\", \"distilbert\", \"distilkobert\", \"xlm-roberta\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            example_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "            start_logits = outputs[0] # bs 512\n",
        "            end_logits = outputs[1] # torch.Tensor\n",
        "                       \n",
        "            # print(len(start_logits)\n",
        "            # print(len(start_logits[0]))\n",
        "            \n",
        "            # print(len(end_logits))\n",
        "            # print(len(end_logits[0]))\n",
        "        \n",
        "\n",
        "        for i, example_index in enumerate(example_indices):\n",
        "            start_logit = start_logits[i].cpu().tolist()\n",
        "            end_logit = end_logits[i].cpu().tolist()\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            # start_logits = start_logits[i]\n",
        "            # end_logits = end_logits[i]\n",
        "            eval_feature = features[example_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "            result = SquadResult(unique_id, start_logit, end_logit)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    logger.info(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n",
        "\n",
        "    # Compute predictions\n",
        "    output_prediction_file = os.path.join(args.output_dir, \"predictions_{}.json\".format(global_step))\n",
        "    output_nbest_file = os.path.join(args.output_dir, \"nbest_predictions_{}.json\".format(global_step))\n",
        "\n",
        "    if args.version_2_with_negative:\n",
        "        output_null_log_odds_file = os.path.join(args.output_dir, \"null_odds_{}.json\".format(global_step))\n",
        "    else:\n",
        "        output_null_log_odds_file = None\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        args.n_best_size,\n",
        "        args.max_answer_length,\n",
        "        args.do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        args.verbose_logging,\n",
        "        args.version_2_with_negative,\n",
        "        args.null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # Compute the F1 and exact scores.\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "    # Write the result\n",
        "    # Write the evaluation result on file\n",
        "    output_dir = os.path.join(args.output_dir, 'eval')\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"eval_result_{}_{}.txt\".format(list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "                                                                               global_step))\n",
        "\n",
        "    with open(output_eval_file, \"w\", encoding='utf-8') as f:\n",
        "        official_eval_results = eval_during_train(args, step=global_step)\n",
        "        results.update(official_eval_results)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False):\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = args.data_dir if args.data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\" if evaluate else \"train\",\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(args.max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", input_dir)\n",
        "\n",
        "        if not args.data_dir and ((evaluate and not args.predict_file) or (not evaluate and not args.train_file)):\n",
        "            try:\n",
        "                import tensorflow_datasets as tfds\n",
        "            except ImportError:\n",
        "                raise ImportError(\"If not data_dir is specified, tensorflow_datasets needs to be installed.\")\n",
        "\n",
        "            if args.version_2_with_negative:\n",
        "                logger.warn(\"tensorflow_datasets does not handle version 2 of SQuAD.\")\n",
        "\n",
        "            tfds_examples = tfds.load(\"squad\")\n",
        "            examples = SquadV1Processor().get_examples_from_dataset(tfds_examples, evaluate=evaluate)\n",
        "        else:\n",
        "            processor = SquadV2Processor() if args.version_2_with_negative else SquadV1Processor()\n",
        "            if evaluate:\n",
        "                examples = processor.get_dev_examples(os.path.join(args.data_dir),\n",
        "                                                      filename=args.predict_file)\n",
        "            else:\n",
        "                examples = processor.get_train_examples(os.path.join(args.data_dir),\n",
        "                                                        filename=args.train_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=args.max_seq_length,\n",
        "            doc_stride=args.doc_stride,\n",
        "            max_query_length=args.max_query_length,\n",
        "            is_training=not evaluate,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=args.threads,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cached_features_file)\n",
        "\n",
        "    if output_examples:\n",
        "        return dataset, examples, features\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # Read from config file and make args\n",
        "    logger.info(\"Training/evaluation parameters {}\".format(args))\n",
        "\n",
        "    args.output_dir = os.path.join(args.output_dir)\n",
        "\n",
        "    if args.doc_stride >= args.max_seq_length - args.max_query_length:\n",
        "        logger.warning(\n",
        "            \"WARNING - You've set a doc stride which may be superior to the document length in some \"\n",
        "            \"examples. This could result in errors when building features from the examples. Please reduce the doc \"\n",
        "            \"stride or increase the maximum length to ensure the features are correctly built.\"\n",
        "        )\n",
        "\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "\n",
        "    logging.getLogger(\"transformers.data.metrics.squad_metrics\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "    )\n",
        "    tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case,\n",
        "    )\n",
        "    model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config,\n",
        "    )\n",
        "    # GPU or CPU\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "    model.to(args.device)\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False, output_examples=False)\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        checkpoints = list(\n",
        "            os.path.dirname(c)\n",
        "            for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce model loading logs\n",
        "\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "\n",
        "        for checkpoint in checkpoints:\n",
        "            # Reload the model\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = MODEL_FOR_QUESTION_ANSWERING[args.model_type].from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluation(args, model, tokenizer, global_step=global_step)\n",
        "            result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
        "                \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWxWjmq8mUFJ"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "## 명령행 인터페이스, 사전 훈련된 모델을 로드하여 훈련하고 평가하는데까지 필요한 인자를 사용자 정의 인자로 설정하는 내용을 담고 있음.\n",
        "## 사용법: python example.py --dataset_name squad.json .... \n",
        "## 주피터 환경을 위한 인자 설정법 \n",
        "# \"output_dir\": \"/content/gdrive/My Drive/Colab Notebooks/DFC615/koelectra-small-korquad-ckpt\",\n",
        "\n",
        "    os.chdir('/content/gdrive/MyDrive/Colab Notebooks/DFC615')\n",
        "    gdrive_path = \"/content/gdrive/My Drive/Colab Notebooks/DFC615\"\n",
        "    data_dir = gdrive_path\n",
        "    output_dir = os.path.join(gdrive_path, \"output\")\n",
        "    # train_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_train.json\")\n",
        "    # dev_file = os.path.join(gdrive_path, \"KorQuAD_v1.0_dev.json\")\n",
        "\n",
        "    hyper_para={\n",
        "    \"task\": \"korquad\",\n",
        "    \"data_dir\": data_dir,\n",
        "    \"ckpt_dir\": \"ckpt\",\n",
        "    \"train_file\": \"KorQuAD_v1.0_train.json\",\n",
        "    \"predict_file\": \"KorQuAD_v1.0_dev.json\",\n",
        "    \"threads\": 4,\n",
        "    \"version_2_with_negative\": False,\n",
        "    \"null_score_diff_threshold\": 0.0,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"doc_stride\": 128,\n",
        "    \"max_query_length\": 64,\n",
        "    \"max_answer_length\": 30,\n",
        "    \"n_best_size\": 20,\n",
        "    \"verbose_logging\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"eval_all_checkpoints\": True,\n",
        "    \"save_optimizer\": False,\n",
        "    \"do_lower_case\": False,\n",
        "    \"do_train\": True,\n",
        "    \"do_eval\": True,\n",
        "    \"num_train_epochs\": 10,\n",
        "    \"weight_decay\": 0.0,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"adam_epsilon\": 1e-8,\n",
        "    \"warmup_proportion\": 0,\n",
        "    \"max_steps\": -1,\n",
        "    \"max_grad_norm\": 1.0,\n",
        "    \"no_cuda\": False,\n",
        "    \"model_type\": \"koelectra-small-v3\",\n",
        "    \"model_name_or_path\": \"monologg/koelectra-small-v3-discriminator\",\n",
        "    \"output_dir\": output_dir,\n",
        "    \"seed\": 42,\n",
        "    \"train_batch_size\": 32,\n",
        "    \"eval_batch_size\": 64,\n",
        "    \"logging_steps\": 3000,\n",
        "    \"save_steps\": 3000,\n",
        "    \"learning_rate\": 5e-5\n",
        "    }\n",
        "\n",
        "    main(AttrDict(hyper_para))\n",
        "\n",
        "   # hyper_para['learning_rate']\n",
        "   # hyper_para.learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9i2vG8-jkX4Z"
      },
      "source": [
        "#2.4 Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psCdEnMzmr6H"
      },
      "source": [
        "test_file= os.path.join(gdrive_path,'test.json')\n",
        "test_json = json.load(open(test_file,'r', encoding='utf-8'))\n",
        "\n",
        "for sample in test_json['data']:\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    print(f'context {context}')\n",
        "    print(f'question {question}')    \n",
        "    print('\\n')\n",
        "    \n",
        "print(len(test_json['data']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phOZGUtrlciK"
      },
      "source": [
        "#prediction 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00mdzI-7m209"
      },
      "source": [
        "tokenizer = ElectraTokenizer.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-6000\")\n",
        "model = ElectraForQuestionAnswering.from_pretrained(\"/content/gdrive/MyDrive/Colab Notebooks/DFC615/output/checkpoint-6000\")\n",
        "prediction_qa = pipeline(\"question-answering\", tokenizer=tokenizer, model=model)\n",
        "\n",
        "context_question = {\n",
        "    'question':'도지코인은 누구의 영향을 받나요?',\n",
        "    'context':'일론머스크의 한마디에 도지코인은 오르락 내리락 오늘도 그의 트위터를 기다린다. 빨간맛 가즈아'\n",
        "    \n",
        "}\n",
        "answer = prediction_qa(context_question)\n",
        "print(answer)\n",
        "answer['answer']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwZz0xOalkDq"
      },
      "source": [
        "정답 끝의 조사만 제거 해서 비교"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWznRFhElaqQ"
      },
      "source": [
        "example_pre = normalize_answer(answer[\"answer\"])\n",
        "print(example_pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_XMR1glxy-"
      },
      "source": [
        "## 2.5 기말고사 제출 리더보드용 파일생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvoeqdJBm_Qv"
      },
      "source": [
        "my_answer = {}\n",
        "my_answer['id']=[]\n",
        "my_answer['prediction_text']=[]\n",
        "\n",
        "for idx, sample in enumerate(test_json['data']):\n",
        "    context = sample['context']\n",
        "    question = sample['question']\n",
        "    answer_dict = prediction_qa({'question':question,'context': context})\n",
        "    predict_text = normalize_answer(answer_dict[\"answer\"])\n",
        "    #predict_text= \"\".join(text_preprocessing(predict_text_1,tokenizer))\n",
        "    \n",
        "    \n",
        "    print(f'context {context}')\n",
        "    print(f'question {question}')\n",
        "    print(f'predict_text {predict_text}')\n",
        "    print('\\n')\n",
        "    \n",
        "    my_answer['id'].append(str(idx + 1))\n",
        "    my_answer['prediction_text'].append(predict_text)\n",
        "\n",
        "df = pd.DataFrame(my_answer)\n",
        "df.to_csv('my_answer.csv', index=False, encoding='utf-8')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc_NPDMlmOQD"
      },
      "source": [
        "결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmLiPQ6ZlvVl"
      },
      "source": [
        "!python /content/gdrive/My Drive/Colab Notebooks/DFC615/evaluate-v1.0.py /content/gdrive/My Drive/Colab Notebooks/DFC615/KorQuAD_v1.0_dev.json /content/gdrive/My Drive/Colab Notebooks/DFC615/output/predictions.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}